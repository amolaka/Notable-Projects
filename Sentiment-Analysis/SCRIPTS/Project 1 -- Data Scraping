#### USED R-STUDIO FOR THIS ####

################ Shogun
library(tidyverse)
library(rvest)

# Function to scrape reviews and ratings from a given URL
scrape_reviews <- function(url) {
  webpage <- read_html(url)
  
  # Extract review titles
  review_titles <- webpage %>%
    html_nodes(".review-container .title") %>%
    html_text(trim = TRUE)
  
  # Extract review contents
  review_contents <- webpage %>%
    html_nodes(".review-container .text.show-more__control") %>%
    html_text(trim = TRUE)
  
  # Extract ratings, if available
  review_ratings <- webpage %>%
    html_nodes(".rating-other-user-rating span") %>%
    html_text(trim = TRUE) %>%
    as.numeric()
    pointslist <- as.list(review_ratings)
    print(pointslist)
    cleaned_points <- Filter(function(x) x != "NA", pointslist)
    print(cleaned_points)
  
      
    
print(review_ratings)
  
  # Ensure all columns have the same length by padding with NA if necessary
  max_length <- max(length(review_titles), length(review_contents), length(review_ratings))
  
  review_titles <- c(review_titles, rep(NA, max_length - length(review_titles)))
  review_contents <- c(review_contents, rep(NA, max_length - length(review_contents)))
  review_ratings <- c(review_ratings, rep(NA, max_length - length(review_ratings)))
  
  data.frame(
    Title = review_titles,
    Content = as.factor(review_contents),
    Rating = as.numeric(cleaned_points)

  )
}

# Initial URL of the IMDb reviews page
initial_url <- "https://www.imdb.com/title/tt2788316/reviews"

# Initialize an empty data frame to store all reviews
all_reviews <- data.frame()

# Set the number of pages you want to scrape (each page contains about 25 reviews)
num_pages <- 28

for (i in 1:num_pages) {
  # Scrape reviews from the current page
  new_reviews <- scrape_reviews(initial_url)
  all_reviews <- bind_rows(all_reviews, new_reviews)
  
  
  # Find the pagination key for the next page
  next_page_key <- read_html(initial_url) %>%
    html_node(".load-more-data") %>%
    html_attr("data-key")
  
  # Check if a "load more" button is present and update the URL if it is
  if (!is.na(next_page_key)) {
    initial_url <- paste0("https://www.imdb.com/title/tt2788316/reviews/_ajax?paginationKey=", next_page_key)
  } else {
    break # Stop if there's no more "load more" button
  }
}


# Get rid of na's
all_reviews<-na.omit(all_reviews)



# Save all reviews to a CSV file
write.csv(all_reviews, "shogun_reviews.csv", row.names = FALSE)

# Print a message indicating completion
cat("Reviews and ratings have been saved to shogun_reviews.csv\n")



################# 3 Body Problem
library(tidyverse)
library(rvest)

# Function to scrape reviews and ratings from a given URL
scrape_reviews <- function(url) {
  webpage <- read_html(url)
  
  # Extract review titles
  review_titles <- webpage %>%
    html_nodes(".review-container .title") %>%
    html_text(trim = TRUE)
  
  # Extract review contents
  review_contents <- webpage %>%
    html_nodes(".review-container .text.show-more__control") %>%
    html_text(trim = TRUE)
  
  # Extract ratings, if available
  review_ratings <- webpage %>%
    html_nodes(".rating-other-user-rating span") %>%
    html_text(trim = TRUE) %>%
    as.numeric()
  pointslist <- as.list(review_ratings)
  print(pointslist)
  cleaned_points <- Filter(function(x) x != "NA", pointslist)
  print(cleaned_points)
  
  
  
  print(review_ratings)
  
  # Ensure all columns have the same length by padding with NA if necessary
  max_length <- max(length(review_titles), length(review_contents), length(review_ratings))
  
  review_titles <- c(review_titles, rep(NA, max_length - length(review_titles)))
  review_contents <- c(review_contents, rep(NA, max_length - length(review_contents)))
  review_ratings <- c(review_ratings, rep(NA, max_length - length(review_ratings)))
  
  data.frame(
    Title = review_titles,
    Content = as.factor(review_contents),
    Rating = as.numeric(cleaned_points)
    
  )
}

# Initial URL of the IMDb reviews page
initial_url <- "https://www.imdb.com/title/tt13016388/reviews"

# Initialize an empty data frame to store all reviews
all_reviews <- data.frame()

# Set the number of pages you want to scrape (each page contains about 25 reviews)
num_pages <- 5665

for (i in 1:num_pages) {
  # Scrape reviews from the current page
  new_reviews <- scrape_reviews(initial_url)
  all_reviews <- bind_rows(all_reviews, new_reviews)
  
  
  # Find the pagination key for the next page
  next_page_key <- read_html(initial_url) %>%
    html_node(".load-more-data") %>%
    html_attr("data-key")
  
  # Check if a "load more" button is present and update the URL if it is
  if (!is.na(next_page_key)) {
    initial_url <- paste0("https://www.imdb.com/title/tt13016388/reviews/_ajax?paginationKey=", next_page_key)
  } else {
    break # Stop if there's no more "load more" button
  }
}


all_reviews<-na.omit(all_reviews)



# Save all reviews to a CSV file
write.csv(all_reviews, "3body_reviews.csv", row.names = FALSE)

# Print a message indicating completion
cat("Reviews and ratings have been saved to 3body_reviews.csv\n")

################ Fallout
library(tidyverse)
library(rvest)

# Function to scrape reviews and ratings from a given URL
scrape_reviews <- function(url) {
  webpage <- read_html(url)
  
  # Extract review titles
  review_titles <- webpage %>%
    html_nodes(".review-container .title") %>%
    html_text(trim = TRUE)
  
  # Extract review contents
  review_contents <- webpage %>%
    html_nodes(".review-container .text.show-more__control") %>%
    html_text(trim = TRUE)
  
  # Extract ratings, if available
  review_ratings <- webpage %>%
    html_nodes(".rating-other-user-rating span") %>%
    html_text(trim = TRUE) %>%
    as.numeric()
  pointslist <- as.list(review_ratings)
  print(pointslist)
  cleaned_points <- Filter(function(x) x != "NA", pointslist)
  print(cleaned_points)
  
  
  
  print(review_ratings)
  
  # Ensure all columns have the same length by padding with NA if necessary
  max_length <- max(length(review_titles), length(review_contents), length(review_ratings))
  
  review_titles <- c(review_titles, rep(NA, max_length - length(review_titles)))
  review_contents <- c(review_contents, rep(NA, max_length - length(review_contents)))
  review_ratings <- c(review_ratings, rep(NA, max_length - length(review_ratings)))
  
  data.frame(
    Title = review_titles,
    Content = as.factor(review_contents),
    Rating = as.numeric(cleaned_points)
    
  )
}

# Initial URL of the IMDb reviews page
initial_url <- "https://www.imdb.com/title/tt12637874/reviews"

# Initialize an empty data frame to store all reviews
all_reviews <- data.frame()

# Set the number of pages you want to scrape (each page contains about 25 reviews)
num_pages <- 45

for (i in 1:num_pages) {
  # Scrape reviews from the current page
  new_reviews <- scrape_reviews(initial_url)
  all_reviews <- bind_rows(all_reviews, new_reviews)
  
  
  # Find the pagination key for the next page
  next_page_key <- read_html(initial_url) %>%
    html_node(".load-more-data") %>%
    html_attr("data-key")
  
  # Check if a "load more" button is present and update the URL if it is
  if (!is.na(next_page_key)) {
    initial_url <- paste0("https://www.imdb.com/title/tt12637874/reviews/_ajax?paginationKey=", next_page_key)
  } else {
    break # Stop if there's no more "load more" button
  }
}


all_reviews<-na.omit(all_reviews)



# Save all reviews to a CSV file
write.csv(all_reviews, "fallout_reviews.csv", row.names = FALSE)

# Print a message indicating completion
cat("Reviews and ratings have been saved to fallout_reviews.csv\n")

############ The Crown
library(tidyverse)
library(rvest)

# Function to scrape reviews and ratings from a given URL
scrape_reviews <- function(url) {
  webpage <- read_html(url)
  
  # Extract review titles
  review_titles <- webpage %>%
    html_nodes(".review-container .title") %>%
    html_text(trim = TRUE)
  
  # Extract review contents
  review_contents <- webpage %>%
    html_nodes(".review-container .text.show-more__control") %>%
    html_text(trim = TRUE)
  
  # Extract ratings, if available
  review_ratings <- webpage %>%
    html_nodes(".rating-other-user-rating span") %>%
    html_text(trim = TRUE) %>%
    as.numeric()
  pointslist <- as.list(review_ratings)
  print(pointslist)
  cleaned_points <- Filter(function(x) x != "NA", pointslist)
  print(cleaned_points)
  
  
  
  print(review_ratings)
  
  # Ensure all columns have the same length by padding with NA if necessary
  max_length <- max(length(review_titles), length(review_contents), length(review_ratings))
  
  review_titles <- c(review_titles, rep(NA, max_length - length(review_titles)))
  review_contents <- c(review_contents, rep(NA, max_length - length(review_contents)))
  review_ratings <- c(review_ratings, rep(NA, max_length - length(review_ratings)))
  
  data.frame(
    Title = review_titles,
    Content = as.factor(review_contents),
    Rating = as.numeric(cleaned_points)
    
  )
}

# Initial URL of the IMDb reviews page
initial_url <- "https://www.imdb.com/title/tt4786824/reviews/"

# Initialize an empty data frame to store all reviews
all_reviews <- data.frame()

# Set the number of pages you want to scrape (each page contains about 25 reviews)
num_pages <- 39

for (i in 1:num_pages) {
  # Scrape reviews from the current page
  new_reviews <- scrape_reviews(initial_url)
  all_reviews <- bind_rows(all_reviews, new_reviews)
  
  
  # Find the pagination key for the next page
  next_page_key <- read_html(initial_url) %>%
    html_node(".load-more-data") %>%
    html_attr("data-key")
  
  # Check if a "load more" button is present and update the URL if it is
  if (!is.na(next_page_key)) {
    initial_url <- paste0("https://www.imdb.com/title/tt4786824/reviews/_ajax?paginationKey=", next_page_key)
  } else {
    break # Stop if there's no more "load more" button
  }
}


all_reviews<-na.omit(all_reviews)



# Save all reviews to a CSV file
write.csv(all_reviews, "the_crown_reviews.csv", row.names = FALSE)

# Print a message indicating completion
cat("Reviews and ratings have been saved to the_crown_reviews.csv\n")

######### mr/mrs_smith
library(tidyverse)
library(rvest)

# Function to scrape reviews and ratings from a given URL
scrape_reviews <- function(url) {
  webpage <- read_html(url)
  
  # Extract review titles
  review_titles <- webpage %>%
    html_nodes(".review-container .title") %>%
    html_text(trim = TRUE)
  
  # Extract review contents
  review_contents <- webpage %>%
    html_nodes(".review-container .text.show-more__control") %>%
    html_text(trim = TRUE)
  
  # Extract ratings, if available
  review_ratings <- webpage %>%
    html_nodes(".rating-other-user-rating span") %>%
    html_text(trim = TRUE) %>%
    as.numeric()
  pointslist <- as.list(review_ratings)
  print(pointslist)
  cleaned_points <- Filter(function(x) x != "NA", pointslist)
  print(cleaned_points)
  
  
  
  print(review_ratings)
  
  # Ensure all columns have the same length by padding with NA if necessary
  max_length <- max(length(review_titles), length(review_contents), length(review_ratings))
  
  review_titles <- c(review_titles, rep(NA, max_length - length(review_titles)))
  review_contents <- c(review_contents, rep(NA, max_length - length(review_contents)))
  review_ratings <- c(review_ratings, rep(NA, max_length - length(review_ratings)))
  
  data.frame(
    Title = review_titles,
    Content = as.factor(review_contents),
    Rating = as.numeric(cleaned_points)
    
  )
}

# Initial URL of the IMDb reviews page
initial_url <- "https://www.imdb.com/title/tt14044212/reviews/"

# Initialize an empty data frame to store all reviews
all_reviews <- data.frame()

# Set the number of pages you want to scrape (each page contains about 25 reviews)
num_pages <- 23

for (i in 1:num_pages) {
  # Scrape reviews from the current page
  new_reviews <- scrape_reviews(initial_url)
  all_reviews <- bind_rows(all_reviews, new_reviews)
  
  
  # Find the pagination key for the next page
  next_page_key <- read_html(initial_url) %>%
    html_node(".load-more-data") %>%
    html_attr("data-key")
  
  # Check if a "load more" button is present and update the URL if it is
  if (!is.na(next_page_key)) {
    initial_url <- paste0("https://www.imdb.com/title/tt14044212/reviews/_ajax?paginationKey=", next_page_key)
  } else {
    break # Stop if there's no more "load more" button
  }
}


all_reviews<-na.omit(all_reviews)



# Save all reviews to a CSV file
write.csv(all_reviews, "mr_mrs_smith_reviews.csv", row.names = FALSE)

# Print a message indicating completion
cat("Reviews and ratings have been saved to mr_mrs_smith_reviews.csv\n")

############ Slow Horses

library(tidyverse)
library(rvest)

# Function to scrape reviews and ratings from a given URL
scrape_reviews <- function(url) {
  webpage <- read_html(url)
  
  # Extract review titles
  review_titles <- webpage %>%
    html_nodes(".review-container .title") %>%
    html_text(trim = TRUE)
  
  # Extract review contents
  review_contents <- webpage %>%
    html_nodes(".review-container .text.show-more__control") %>%
    html_text(trim = TRUE)
  
  # Extract ratings, if available
  review_ratings <- webpage %>%
    html_nodes(".rating-other-user-rating span") %>%
    html_text(trim = TRUE) %>%
    as.numeric()
  pointslist <- as.list(review_ratings)
  print(pointslist)
  cleaned_points <- Filter(function(x) x != "NA", pointslist)
  print(cleaned_points)
  
  
  
  print(review_ratings)
  
  # Ensure all columns have the same length by padding with NA if necessary
  max_length <- max(length(review_titles), length(review_contents), length(review_ratings))
  
  review_titles <- c(review_titles, rep(NA, max_length - length(review_titles)))
  review_contents <- c(review_contents, rep(NA, max_length - length(review_contents)))
  review_ratings <- c(review_ratings, rep(NA, max_length - length(review_ratings)))
  
  data.frame(
    Title = review_titles,
    Content = as.factor(review_contents),
    Rating = as.numeric(cleaned_points)
    
  )
}

# Initial URL of the IMDb reviews page
initial_url <- "https://www.imdb.com/title/tt5875444/reviews"

# Initialize an empty data frame to store all reviews
all_reviews <- data.frame()

# Set the number of pages you want to scrape (each page contains about 25 reviews)
num_pages <- 14

for (i in 1:num_pages) {
  # Scrape reviews from the current page
  new_reviews <- scrape_reviews(initial_url)
  all_reviews <- bind_rows(all_reviews, new_reviews)
  
  
  # Find the pagination key for the next page
  next_page_key <- read_html(initial_url) %>%
    html_node(".load-more-data") %>%
    html_attr("data-key")
  
  # Check if a "load more" button is present and update the URL if it is
  if (!is.na(next_page_key)) {
    initial_url <- paste0("https://www.imdb.com/title/tt5875444/reviews/_ajax?paginationKey=", next_page_key)
  } else {
    break # Stop if there's no more "load more" button
  }
}


all_reviews<-na.omit(all_reviews)



# Save all reviews to a CSV file
write.csv(all_reviews, "slow_horses_reviews.csv", row.names = FALSE)

# Print a message indicating completion
cat("Reviews and ratings have been saved to slow_horses_reviews.csv\n")


########### The Gilded Age

library(tidyverse)
library(rvest)

# Function to scrape reviews and ratings from a given URL
scrape_reviews <- function(url) {
  webpage <- read_html(url)
  
  # Extract review titles
  review_titles <- webpage %>%
    html_nodes(".review-container .title") %>%
    html_text(trim = TRUE)
  
  # Extract review contents
  review_contents <- webpage %>%
    html_nodes(".review-container .text.show-more__control") %>%
    html_text(trim = TRUE)
  
  # Extract ratings, if available
  review_ratings <- webpage %>%
    html_nodes(".rating-other-user-rating span") %>%
    html_text(trim = TRUE) %>%
    as.numeric()
  pointslist <- as.list(review_ratings)
  print(pointslist)
  cleaned_points <- Filter(function(x) x != "NA", pointslist)
  print(cleaned_points)
  
  
  
  print(review_ratings)
  
  # Ensure all columns have the same length by padding with NA if necessary
  max_length <- max(length(review_titles), length(review_contents), length(review_ratings))
  
  review_titles <- c(review_titles, rep(NA, max_length - length(review_titles)))
  review_contents <- c(review_contents, rep(NA, max_length - length(review_contents)))
  review_ratings <- c(review_ratings, rep(NA, max_length - length(review_ratings)))
  
  data.frame(
    Title = review_titles,
    Content = as.factor(review_contents),
    Rating = as.numeric(cleaned_points)
    
  )
}

# Initial URL of the IMDb reviews page
initial_url <- "https://www.imdb.com/title/tt4406178/reviews"

# Initialize an empty data frame to store all reviews
all_reviews <- data.frame()

# Set the number of pages you want to scrape (each page contains about 25 reviews)
num_pages <- 18

for (i in 1:num_pages) {
  # Scrape reviews from the current page
  new_reviews <- scrape_reviews(initial_url)
  all_reviews <- bind_rows(all_reviews, new_reviews)
  
  
  # Find the pagination key for the next page
  next_page_key <- read_html(initial_url) %>%
    html_node(".load-more-data") %>%
    html_attr("data-key")
  
  # Check if a "load more" button is present and update the URL if it is
  if (!is.na(next_page_key)) {
    initial_url <- paste0("https://www.imdb.com/title/tt4406178/reviews/_ajax?paginationKey=", next_page_key)
  } else {
    break # Stop if there's no more "load more" button
  }
}


all_reviews<-na.omit(all_reviews)



# Save all reviews to a CSV file
write.csv(all_reviews, "the_gilded_reviews.csv", row.names = FALSE)

# Print a message indicating completion
cat("Reviews and ratings have been saved to the_gilded_reviews.csv\n")

############## The Morning Show
library(tidyverse)
library(rvest)

# Function to scrape reviews and ratings from a given URL
scrape_reviews <- function(url) {
  webpage <- read_html(url)
  
  # Extract review titles
  review_titles <- webpage %>%
    html_nodes(".review-container .title") %>%
    html_text(trim = TRUE)
  
  # Extract review contents
  review_contents <- webpage %>%
    html_nodes(".review-container .text.show-more__control") %>%
    html_text(trim = TRUE)
  
  # Extract ratings, if available
  review_ratings <- webpage %>%
    html_nodes(".rating-other-user-rating span") %>%
    html_text(trim = TRUE) %>%
    as.numeric()
  pointslist <- as.list(review_ratings)
  print(pointslist)
  cleaned_points <- Filter(function(x) x != "NA", pointslist)
  print(cleaned_points)
  
  
  
  print(review_ratings)
  
  # Ensure all columns have the same length by padding with NA if necessary
  max_length <- max(length(review_titles), length(review_contents), length(review_ratings))
  
  review_titles <- c(review_titles, rep(NA, max_length - length(review_titles)))
  review_contents <- c(review_contents, rep(NA, max_length - length(review_contents)))
  review_ratings <- c(review_ratings, rep(NA, max_length - length(review_ratings)))
  
  data.frame(
    Title = review_titles,
    Content = as.factor(review_contents),
    Rating = as.numeric(cleaned_points)
    
  )
}

# Initial URL of the IMDb reviews page
initial_url <- "https://www.imdb.com/title/tt7203552/reviews/"

# Initialize an empty data frame to store all reviews
all_reviews <- data.frame()

# Set the number of pages you want to scrape (each page contains about 25 reviews)
num_pages <- 43

for (i in 1:num_pages) {
  # Scrape reviews from the current page
  new_reviews <- scrape_reviews(initial_url)
  all_reviews <- bind_rows(all_reviews, new_reviews)
  
  
  # Find the pagination key for the next page
  next_page_key <- read_html(initial_url) %>%
    html_node(".load-more-data") %>%
    html_attr("data-key")
  
  # Check if a "load more" button is present and update the URL if it is
  if (!is.na(next_page_key)) {
    initial_url <- paste0("https://www.imdb.com/title/tt7203552/reviews/_ajax?paginationKey=", next_page_key)
  } else {
    break # Stop if there's no more "load more" button
  }
}


all_reviews<-na.omit(all_reviews)



# Save all reviews to a CSV file
write.csv(all_reviews, "the_morning_show_reviews.csv", row.names = FALSE)

# Print a message indicating completion
cat("Reviews and ratings have been saved to the_morning_show_reviews.csv\n")





































































